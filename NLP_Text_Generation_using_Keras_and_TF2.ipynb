{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Text  Generation using Keras and TF2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQRkwsXq4peq44SwxVpNhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishast/TF/blob/master/NLP_Text_Generation_using_Keras_and_TF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFG7rx6Nw-3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlaONDV4KFwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mp\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqz6mlmmwkhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqa3gLtoKcGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps7AMT85Kiy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = \"shakespeare.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5HliIzFLNUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(path_to_file,'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-3PW7wLpx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text[2999:30100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itl4Lq-jLuv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(set(text)) # Return all the unique characters in the text file in a sorted manner. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1isAwH9EN6i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text)) #contains all the characters in the text  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1RLnjd9Oznc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(vocab) # count the number of characters in the text file. This will be used at the output layer of the RNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Hj_KmFO1fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for pair in enumerate(vocab):\n",
        "  print (pair)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ki-LwF5P4-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOHCmITxQpGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind_to_char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX8T7drqRkOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind_to_char[80]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjCcYQOSK4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU43ITALSqbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIc6dwK0TCxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text[2:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Bokmm3URZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text[2:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSPnKqR9dBuT",
        "colab_type": "text"
      },
      "source": [
        "### Defining the sequence length from the lines. To find the structure of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmgK0sNUW8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (text[:300])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzcKdbEDctea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line = \"From fairest creatures we desire increase\" #Taking a single line \n",
        "lines = '''\n",
        "From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ivyBjIdQ7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (len(line))\n",
        "print (len (lines))\n",
        "# Here a structured line from the poem or suanet here is of 133 line hence taking it to be of 120 line. \n",
        "#This can change for different type of texts. A story or an email can have different sequence legnth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KP-HFC7dRrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3kPE3XeCjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_num_sep = print (len(text)// seq_len + 1)  # Finding the total sequences in the text. Also using + 1 as 0 is the indexing. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH-0X0-xeO1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_dateset = tf.data.Dataset.from_tensor_slices(encoded_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MAFJMc5fWdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(char_dateset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmJPZk8PfZI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (char_dateset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDqmKQRcfcDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in char_dateset.take(500):\n",
        "  print(ind_to_char[item.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvloL1dgQuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dateset.batch(batch_size=seq_len+1,drop_remainder=True)\n",
        "#creating a sequence batches of length 121 from the dataset. Dropping the remaining characters from the text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYLpitm6I9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, creating a target sequences from sequence above by shifting each batch by +1 and storing in tuple object."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxLGPitr6KDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "  input_txt = seq[:-1] # \"Hello my nam...\"\n",
        "  output_txt = seq[1:] # \"ello my name....\"\n",
        "  return input_txt,output_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNMfhMi8928v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5chKYg0898Ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_txt, target_txt in dataset.take(1):\n",
        "  print(input_txt.numpy())\n",
        "  print (\"\".join(ind_to_char[input_txt.numpy()])) ### This is the input sequence \n",
        "  print ('\\n')\n",
        "  print (target_txt.numpy())\n",
        "  print (\"\".join(ind_to_char[target_txt.numpy()])) # This is the target sequence \n",
        "  print ('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4V_fkpYhf1a",
        "colab_type": "text"
      },
      "source": [
        "### Next target is to create or generate the Training sequence. \n",
        "Batches and shuffle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WakxkRTs-zRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "batch_size = 128\n",
        "\n",
        "# Need to define the buffer size for also for randomamising. Else all the text in the memory can be randsomised.\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6xLcW7VkerQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je0KN3AClDCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "rnn_neurons=1026\n",
        "embeded_dim=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhgkLcIjmVYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CXifrLimNE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpZLL0xJm6lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y-CJZgcmrcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocab_size,embeded_dim,rnn_neurons,batch_size):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,embeded_dim,batch_input_shape=[batch_size,None]))\n",
        "  model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
        "              recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  model.add(Dense(vocab_size))\n",
        "\n",
        "  model.compile('adam',loss=sparse_cat_loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mAyaIzBpes_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(vocab_size=vocab_size,embeded_dim=embeded_dim,rnn_neurons=rnn_neurons,\n",
        "                     batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaP3OJwrpolS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-cfK2doefQl",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ONd-OMpwoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  #input_example_batch is the first sequence of letters\n",
        "  #target_example_batch is ths initial sequence shifted just one  \n",
        "  example_batch_predections = model(input_example_batch)\n",
        "  #print (tf.shape(input_example_batch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUSs9QCNfVFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch_predections.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2-6J_wm_hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_batch_predections[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R24boWPUn0sS",
        "colab_type": "text"
      },
      "source": [
        "## Grabbing the indices from the tensor and finding the sequence of number generated by the predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V58TXMUpnCgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sample_indices = tf.random.categorical(example_batch_predections[0],num_samples=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy5ClFZUoC2m",
        "colab_type": "text"
      },
      "source": [
        "## Next with the shape of this tensor, we need to convert it to an array to check out the char by passing in ind_to_char array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DlF93fDnPQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_indices = tf.squeeze(sample_indices,axis =-1).numpy()\n",
        "## reshaping into a numpy array by using squeeze method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv3Y9ON9nViw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XZo5R2lolK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind_to_char[sample_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRRMC3ANhH0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qf2iglTSMdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## NEw code\n",
        "\n",
        "import os \n",
        "checkpoint_path = \"training_1\\cp.ckpt\" \n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        " # Create checkpoint  callback \n",
        "checkpoint_callback =tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_weights_only=True,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpgvBFPopWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4F3bClpH-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUOIq6b1hz9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GyOXSvjf8-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "model = create_model(vocab_size=vocab_size,embeded_dim=embeded_dim,rnn_neurons=rnn_neurons,\n",
        "                     batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v1wMkfaWZ6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(model)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1pd8GPnf87u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQUwwFSpMhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters.\n",
        "  '''\n",
        "\n",
        "  # defining the number of elements we want to predict\n",
        "  num_generate = gen_size\n",
        "  # converting the charater in starting seed string into characters.\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "  \n",
        "  #changing the dimentsion so that its in right shape for our model.'\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "  #empty list to append the text generated by the model\n",
        "  text_generated = []\n",
        "\n",
        "  #Need to set the temperature for the randomness in our model.\n",
        "  #The temeprerature divdes the logits which will be used for the softmax function before passing onto the next\n",
        "  temperature = temp\n",
        "\n",
        "  #Resetting the state of model before calling calling the predection clears the hidden layer states.\n",
        "  model.reset_states()\n",
        "\n",
        "  #Looping in through each of the characters\n",
        "  for i in range(num_generate): \n",
        "\n",
        "    prediction = model(input_eval)\n",
        "\n",
        "    #squezzing to undo the expanding dimensions domne above.\n",
        "    prediction = tf.squeeze(prediction,0)\n",
        "\n",
        "    #dividing the logits\n",
        "    prediction = prediction/temperature\n",
        "\n",
        "    #finding the single digit from the tensor to find the index and pass ot character array to find the real letters.\n",
        "    #use function random.categorical\n",
        "\n",
        "    predicted_id = tf.random.categorical(prediction,num_samples=1)[-1:0].numpy()\n",
        "\n",
        "    #Expand the predected text \n",
        "    input_eval = tf.expand_dims(predicted_id,0)\n",
        "\n",
        "    #append into the list\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + \"\".join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB4JIQXJP4f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model,\"BRUTU\",gen_size=500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9AeM30YQHHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
        "  time series problems.\n",
        "  '''\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  # Temperature effects randomness in our resulting text\n",
        "  # The term is derived from entropy/thermodynamics.\n",
        "  # The temperature is used to effect probability of next characters.\n",
        "  # Higher probability == lesss surprising/ more expected\n",
        "  # Lower temperature == more surprising / less expected\n",
        " \n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      print (tf.shape(input_eval))\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHeYi6tZQ5Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model,\"BRUTU\",gen_size=500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pz3EUO_Q9qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}