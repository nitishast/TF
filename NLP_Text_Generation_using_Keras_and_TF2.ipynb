{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Text  Generation using Keras and TF2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQRkwsXq4peq44SwxVpNhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishast/TF/blob/master/NLP_Text_Generation_using_Keras_and_TF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFG7rx6Nw-3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlaONDV4KFwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mp\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqz6mlmmwkhw",
        "colab_type": "code",
        "outputId": "1f1c193a-6822-4841-c0a0-e51efb5f44fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqa3gLtoKcGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps7AMT85Kiy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = \"shakespeare.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5HliIzFLNUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(path_to_file,'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-3PW7wLpx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "42435cc5-0f52-484d-92e2-bf3f75b6fae2"
      },
      "source": [
        "text[2999:30100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ne,\\n  Beauty o'er-snowed and bareness every where:\\n  Then were not summer's distillation left\\n  A liquid prisoner pent in walls of glass,\\n  Beauty's effect with beauty were bereft,\\n  Nor it nor no remembrance what it was.\\n    But flowers distilled though they with winter meet,\\n    Leese but their show, their substance still lives sweet.\\n\\n\\n                     6  \\n  Then let not winter's ragged hand deface,\\n  In thee thy summer ere thou be distilled:\\n  Make sweet some vial; treasure thou some place,\\n  With beauty's treasure ere it be self-killed:\\n  That use is not forbidden usury,\\n  Which happies those that pay the willing loan;\\n  That's for thy self to breed another thee,\\n  Or ten times happier be it ten for one,\\n  Ten times thy self were happier than thou art,\\n  If ten of thine ten times refigured thee:\\n  Then what could death do if thou shouldst depart,\\n  Leaving thee living in posterity?\\n    Be not self-willed for thou art much too fair,\\n    To be death's conquest and make worms thine heir.\\n\\n\\n                     7\\n  Lo in the orient when the gracious light\\n  Lifts up his burning head, each under eye\\n  Doth homage to his new-appearing sight,\\n  Serving with looks his sacred majesty,  \\n  And having climbed the steep-up heavenly hill,\\n  Resembling strong youth in his middle age,\\n  Yet mortal looks adore his beauty still,\\n  Attending on his golden pilgrimage:\\n  But when from highmost pitch with weary car,\\n  Like feeble age he reeleth from the day,\\n  The eyes (fore duteous) now converted are\\n  From his low tract and look another way:\\n    So thou, thy self out-going in thy noon:\\n    Unlooked on diest unless thou get a son.\\n\\n\\n                     8\\n  Music to hear, why hear'st thou music sadly?\\n  Sweets with sweets war not, joy delights in joy:\\n  Why lov'st thou that which thou receiv'st not gladly,\\n  Or else receiv'st with pleasure thine annoy?\\n  If the true concord of well-tuned sounds,\\n  By unions married do offend thine ear,\\n  They do but sweetly chide thee, who confounds\\n  In singleness the parts that thou shouldst bear:  \\n  Mark how one string sweet husband to another,\\n  Strikes each in each by mutual ordering;\\n  Resembling sire, and child, and happy mother,\\n  Who all in one, one pleasing note do sing:\\n    Whose speechless song being many, seeming one,\\n    Sings this to thee, 'Thou single wilt prove none'.\\n\\n\\n                     9\\n  Is it for fear to wet a widow's eye,\\n  That thou consum'st thy self in single life?\\n  Ah, if thou issueless shalt hap to die,\\n  The world will wail thee like a makeless wife,\\n  The world will be thy widow and still weep,\\n  That thou no form of thee hast left behind,\\n  When every private widow well may keep,\\n  By children's eyes, her husband's shape in mind:\\n  Look what an unthrift in the world doth spend\\n  Shifts but his place, for still the world enjoys it;\\n  But beauty's waste hath in the world an end,\\n  And kept unused the user so destroys it:  \\n    No love toward others in that bosom sits\\n    That on himself such murd'rous shame commits.\\n\\n\\n                     10\\n  For shame deny that thou bear'st love to any\\n  Who for thy self art so unprovident.\\n  Grant if thou wilt, thou art beloved of many,\\n  But that thou none lov'st is most evident:\\n  For thou art so possessed with murd'rous hate,\\n  That 'gainst thy self thou stick'st not to conspire,\\n  Seeking that beauteous roof to ruinate\\n  Which to repair should be thy chief desire:\\n  O change thy thought, that I may change my mind,\\n  Shall hate be fairer lodged than gentle love?\\n  Be as thy presence is gracious and kind,\\n  Or to thy self at least kind-hearted prove,\\n    Make thee another self for love of me,\\n    That beauty still may live in thine or thee.\\n\\n\\n                     11  \\n  As fast as thou shalt wane so fast thou grow'st,\\n  In one of thine, from that which thou departest,\\n  And that fresh blood which youngly thou bestow'st,\\n  Thou mayst call thine, when thou from youth convertest,\\n  Herein lives wisdom, beauty, and increase,\\n  Without this folly, age, and cold decay,\\n  If all were minded so, the times should cease,\\n  And threescore year would make the world away:\\n  Let those whom nature hath not made for store,\\n  Harsh, featureless, and rude, barrenly perish:\\n  Look whom she best endowed, she gave thee more;\\n  Which bounteous gift thou shouldst in bounty cherish:\\n    She carved thee for her seal, and meant thereby,\\n    Thou shouldst print more, not let that copy die.\\n\\n\\n                     12\\n  When I do count the clock that tells the time,\\n  And see the brave day sunk in hideous night,\\n  When I behold the violet past prime,\\n  And sable curls all silvered o'er with white:  \\n  When lofty trees I see barren of leaves,\\n  Which erst from heat did canopy the herd\\n  And summer's green all girded up in sheaves\\n  Borne on the bier with white and bristly beard:\\n  Then of thy beauty do I question make\\n  That thou among the wastes of time must go,\\n  Since sweets and beauties do themselves forsake,\\n  And die as fast as they see others grow,\\n    And nothing 'gainst Time's scythe can make defence\\n    Save breed to brave him, when he takes thee hence.\\n\\n\\n                     13\\n  O that you were your self, but love you are\\n  No longer yours, than you your self here live,\\n  Against this coming end you should prepare,\\n  And your sweet semblance to some other give.\\n  So should that beauty which you hold in lease\\n  Find no determination, then you were\\n  Your self again after your self's decease,\\n  When your sweet issue your sweet form should bear.  \\n  Who lets so fair a house fall to decay,\\n  Which husbandry in honour might uphold,\\n  Against the stormy gusts of winter's day\\n  And barren rage of death's eternal cold?\\n    O none but unthrifts, dear my love you know,\\n    You had a father, let your son say so.\\n\\n\\n                     14\\n  Not from the stars do I my judgement pluck,\\n  And yet methinks I have astronomy,\\n  But not to tell of good, or evil luck,\\n  Of plagues, of dearths, or seasons' quality,\\n  Nor can I fortune to brief minutes tell;\\n  Pointing to each his thunder, rain and wind,\\n  Or say with princes if it shall go well\\n  By oft predict that I in heaven find.\\n  But from thine eyes my knowledge I derive,\\n  And constant stars in them I read such art\\n  As truth and beauty shall together thrive\\n  If from thy self, to store thou wouldst convert:  \\n    Or else of thee this I prognosticate,\\n    Thy end is truth's and beauty's doom and date.\\n\\n\\n                     15\\n  When I consider every thing that grows\\n  Holds in perfection but a little moment.\\n  That this huge stage presenteth nought but shows\\n  Whereon the stars in secret influence comment.\\n  When I perceive that men as plants increase,\\n  Cheered and checked even by the self-same sky:\\n  Vaunt in their youthful sap, at height decrease,\\n  And wear their brave state out of memory.\\n  Then the conceit of this inconstant stay,\\n  Sets you most rich in youth before my sight,\\n  Where wasteful time debateth with decay\\n  To change your day of youth to sullied night,\\n    And all in war with Time for love of you,\\n    As he takes from you, I engraft you new.\\n\\n\\n                     16  \\n  But wherefore do not you a mightier way\\n  Make war upon this bloody tyrant Time?\\n  And fortify your self in your decay\\n  With means more blessed than my barren rhyme?\\n  Now stand you on the top of happy hours,\\n  And many maiden gardens yet unset,\\n  With virtuous wish would bear you living flowers,\\n  Much liker than your painted counterfeit:\\n  So should the lines of life that life repair\\n  Which this (Time's pencil) or my pupil pen\\n  Neither in inward worth nor outward fair\\n  Can make you live your self in eyes of men.\\n    To give away your self, keeps your self still,\\n    And you must live drawn by your own sweet skill.\\n\\n\\n                     17\\n  Who will believe my verse in time to come\\n  If it were filled with your most high deserts?\\n  Though yet heaven knows it is but as a tomb\\n  Which hides your life, and shows not half your parts:  \\n  If I could write the beauty of your eyes,\\n  And in fresh numbers number all your graces,\\n  The age to come would say this poet lies,\\n  Such heavenly touches ne'er touched earthly faces.\\n  So should my papers (yellowed with their age)\\n  Be scorned, like old men of less truth than tongue,\\n  And your true rights be termed a poet's rage,\\n  And stretched metre of an antique song.\\n    But were some child of yours alive that time,\\n    You should live twice in it, and in my rhyme.\\n\\n\\n                     18\\n  Shall I compare thee to a summer's day?\\n  Thou art more lovely and more temperate:\\n  Rough winds do shake the darling buds of May,\\n  And summer's lease hath all too short a date:\\n  Sometime too hot the eye of heaven shines,\\n  And often is his gold complexion dimmed,\\n  And every fair from fair sometime declines,\\n  By chance, or nature's changing course untrimmed:  \\n  But thy eternal summer shall not fade,\\n  Nor lose possession of that fair thou ow'st,\\n  Nor shall death brag thou wand'rest in his shade,\\n  When in eternal lines to time thou grow'st,\\n    So long as men can breathe or eyes can see,\\n    So long lives this, and this gives life to thee.\\n\\n\\n                     19\\n  Devouring Time blunt thou the lion's paws,\\n  And make the earth devour her own sweet brood,\\n  Pluck the keen teeth from the fierce tiger's jaws,\\n  And burn the long-lived phoenix, in her blood,\\n  Make glad and sorry seasons as thou fleet'st,\\n  And do whate'er thou wilt swift-footed Time\\n  To the wide world and all her fading sweets:\\n  But I forbid thee one most heinous crime,\\n  O carve not with thy hours my love's fair brow,\\n  Nor draw no lines there with thine antique pen,\\n  Him in thy course untainted do allow,\\n  For beauty's pattern to succeeding men.  \\n    Yet do thy worst old Time: despite thy wrong,\\n    My love shall in my verse ever live young.\\n\\n\\n                     20\\n  A woman's face with nature's own hand painted,\\n  Hast thou the master mistress of my passion,\\n  A woman's gentle heart but not acquainted\\n  With shifting change as is false women's fashion,\\n  An eye more bright than theirs, less false in rolling:\\n  Gilding the object whereupon it gazeth,\\n  A man in hue all hues in his controlling,\\n  Which steals men's eyes and women's souls amazeth.\\n  And for a woman wert thou first created,\\n  Till nature as she wrought thee fell a-doting,\\n  And by addition me of thee defeated,\\n  By adding one thing to my purpose nothing.\\n    But since she pricked thee out for women's pleasure,\\n    Mine be thy love and thy love's use their treasure.\\n\\n\\n                     21  \\n  So is it not with me as with that muse,\\n  Stirred by a painted beauty to his verse,\\n  Who heaven it self for ornament doth use,\\n  And every fair with his fair doth rehearse,\\n  Making a couplement of proud compare\\n  With sun and moon, with earth and sea's rich gems:\\n  With April's first-born flowers and all things rare,\\n  That heaven's air in this huge rondure hems.\\n  O let me true in love but truly write,\\n  And then believe me, my love is as fair,\\n  As any mother's child, though not so bright\\n  As those gold candles fixed in heaven's air:\\n    Let them say more that like of hearsay well,\\n    I will not praise that purpose not to sell.\\n\\n\\n                     22\\n  My glass shall not persuade me I am old,\\n  So long as youth and thou are of one date,\\n  But when in thee time's furrows I behold,\\n  Then look I death my days should expiate.  \\n  For all that beauty that doth cover thee,\\n  Is but the seemly raiment of my heart,\\n  Which in thy breast doth live, as thine in me,\\n  How can I then be elder than thou art?\\n  O therefore love be of thyself so wary,\\n  As I not for my self, but for thee will,\\n  Bearing thy heart which I will keep so chary\\n  As tender nurse her babe from faring ill.\\n    Presume not on thy heart when mine is slain,\\n    Thou gav'st me thine not to give back again.\\n\\n\\n                     23\\n  As an unperfect actor on the stage,\\n  Who with his fear is put beside his part,\\n  Or some fierce thing replete with too much rage,\\n  Whose strength's abundance weakens his own heart;\\n  So I for fear of trust, forget to say,\\n  The perfect ceremony of love's rite,\\n  And in mine own love's strength seem to decay,\\n  O'ercharged with burthen of mine own love's might:  \\n  O let my looks be then the eloquence,\\n  And dumb presagers of my speaking breast,\\n  Who plead for love, and look for recompense,\\n  More than that tongue that more hath more expressed.\\n    O learn to read what silent love hath writ,\\n    To hear with eyes belongs to love's fine wit.\\n\\n\\n                     24\\n  Mine eye hath played the painter and hath stelled,\\n  Thy beauty's form in table of my heart,\\n  My body is the frame wherein 'tis held,\\n  And perspective it is best painter's art.\\n  For through the painter must you see his skill,\\n  To find where your true image pictured lies,\\n  Which in my bosom's shop is hanging still,\\n  That hath his windows glazed with thine eyes:\\n  Now see what good turns eyes for eyes have done,\\n  Mine eyes have drawn thy shape, and thine for me\\n  Are windows to my breast, where-through the sun\\n  Delights to peep, to gaze therein on thee;  \\n    Yet eyes this cunning want to grace their art,\\n    They draw but what they see, know not the heart.\\n\\n\\n                     25\\n  Let those who are in favour with their stars,\\n  Of public honour and proud titles boast,\\n  Whilst I whom fortune of such triumph bars\\n  Unlooked for joy in that I honour most;\\n  Great princes' favourites their fair leaves spread,\\n  But as the marigold at the sun's eye,\\n  And in themselves their pride lies buried,\\n  For at a frown they in their glory die.\\n  The painful warrior famoused for fight,\\n  After a thousand victories once foiled,\\n  Is from the book of honour razed quite,\\n  And all the rest forgot for which he toiled:\\n    Then happy I that love and am beloved\\n    Where I may not remove nor be removed.\\n\\n\\n                     26  \\n  Lord of my love, to whom in vassalage\\n  Thy merit hath my duty strongly knit;\\n  To thee I send this written embassage\\n  To witness duty, not to show my wit.\\n  Duty so great, which wit so poor as mine\\n  May make seem bare, in wanting words to show it;\\n  But that I hope some good conceit of thine\\n  In thy soul's thought (all naked) will bestow it:\\n  Till whatsoever star that guides my moving,\\n  Points on me graciously with fair aspect,\\n  And puts apparel on my tattered loving,\\n  To show me worthy of thy sweet respect,\\n    Then may I dare to boast how I do love thee,\\n    Till then, not show my head where thou mayst prove me.\\n\\n\\n                     27\\n  Weary with toil, I haste me to my bed,\\n  The dear respose for limbs with travel tired,\\n  But then begins a journey in my head\\n  To work my mind, when body's work's expired.  \\n  For then my thoughts (from far where I abide)\\n  Intend a zealous pilgrimage to thee,\\n  And keep my drooping eyelids open wide,\\n  Looking on darkness which the blind do see.\\n  Save that my soul's imaginary sight\\n  Presents thy shadow to my sightless view,\\n  Which like a jewel (hung in ghastly night)\\n  Makes black night beauteous, and her old face new.\\n    Lo thus by day my limbs, by night my mind,\\n    For thee, and for my self, no quiet find.\\n\\n\\n                     28\\n  How can I then return in happy plight\\n  That am debarred the benefit of rest?\\n  When day's oppression is not eased by night,\\n  But day by night and night by day oppressed.\\n  And each (though enemies to either's reign)\\n  Do in consent shake hands to torture me,\\n  The one by toil, the other to complain\\n  How far I toil, still farther off from thee.  \\n  I tell the day to please him thou art bright,\\n  And dost him grace when clouds do blot the heaven:\\n  So flatter I the swart-complexioned night,\\n  When sparkling stars twire not thou gild'st the even.\\n    But day doth daily draw my sorrows longer,\\n    And night doth nightly make grief's length seem stronger\\n\\n\\n                     29\\n  When in disgrace with Fortune and men's eyes,\\n  I all alone beweep my outcast state,\\n  And trouble deaf heaven with my bootless cries,\\n  And look upon my self and curse my fate,\\n  Wishing me like to one more rich in hope,\\n  Featured like him, like him with friends possessed,\\n  Desiring this man's art, and that man's scope,\\n  With what I most enjoy contented least,\\n  Yet in these thoughts my self almost despising,\\n  Haply I think on thee, and then my state,\\n  (Like to the lark at break of day arising\\n  From sullen earth) sings hymns at heaven's gate,  \\n    For thy sweet love remembered such wealth brings,\\n    That then I scorn to change my state with kings.\\n\\n\\n                     30\\n  When to the sessions of sweet silent thought,\\n  I summon up remembrance of things past,\\n  I sigh the lack of many a thing I sought,\\n  And with old woes new wail my dear time's waste:\\n  Then can I drown an eye (unused to flow)\\n  For precious friends hid in death's dateless night,\\n  And weep afresh love's long since cancelled woe,\\n  And moan th' expense of many a vanished sight.\\n  Then can I grieve at grievances foregone,\\n  And heavily from woe to woe tell o'er\\n  The sad account of fore-bemoaned moan,\\n  Which I new pay as if not paid before.\\n    But if the while I think on thee (dear friend)\\n    All losses are restored, and sorrows end.\\n\\n\\n                     31  \\n  Thy bosom is endeared with all hearts,\\n  Which I by lacking have supposed dead,\\n  And there reigns love and all love's loving parts,\\n  And all those friends which I thought buried.\\n  How many a holy and obsequious tear\\n  Hath dear religious love stol'n from mine eye,\\n  As interest of the dead, which now appear,\\n  But things removed that hidden in thee lie.\\n  Thou art the grave where buried love doth live,\\n  Hung with the trophies of my lovers gone,\\n  Who all their parts of me to thee did give,\\n  That due of many, now is thine alone.\\n    Their images I loved, I view in thee,\\n    And thou (all they) hast all the all of me.\\n\\n\\n                     32\\n  If thou survive my well-contented day,\\n  When that churl death my bones with dust shall cover\\n  And shalt by fortune once more re-survey\\n  These poor rude lines of thy deceased lover:  \\n  Compare them with the bett'ring of the time,\\n  And though they be outstripped by every pen,\\n  Reserve them for my love, not for their rhyme,\\n  Exceeded by the height of happier men.\\n  O then vouchsafe me but this loving thought,\\n  'Had my friend's Muse grown with this growing age,\\n  A dearer birth than this his love had brought\\n  To march in ranks of better equipage:\\n    But since he died and poets better prove,\\n    Theirs for their style I'll read, his for his love'.\\n\\n\\n                     33\\n  Full many a glorious morning have I seen,\\n  Flatter the mountain tops with sovereign eye,\\n  Kissing with golden face the meadows green;\\n  Gilding pale streams with heavenly alchemy:\\n  Anon permit the basest clouds to ride,\\n  With ugly rack on his celestial face,\\n  And from the forlorn world his visage hide\\n  Stealing unseen to west with this disgrace:  \\n  Even so my sun one early morn did shine,\\n  With all triumphant splendour on my brow,\\n  But out alack, he was but one hour mine,\\n  The region cloud hath masked him from me now.\\n    Yet him for this, my love no whit disdaineth,\\n    Suns of the world may stain, when heaven's sun staineth.\\n\\n\\n                     34\\n  Why didst thou promise such a beauteous day,\\n  And make me travel forth without my cloak,\\n  To let base clouds o'ertake me in my way,\\n  Hiding thy brav'ry in their rotten smoke?\\n  'Tis not enough that through the cloud thou break,\\n  To dry the rain on my storm-beaten face,\\n  For no man well of such a salve can speak,\\n  That heals the wound, and cures not the disgrace:\\n  Nor can thy shame give physic to my grief,\\n  Though thou repent, yet I have still the loss,\\n  Th' offender's sorrow lends but weak relief\\n  To him that bears the strong offence's cross.  \\n    Ah but those tears are pearl which thy love sheds,\\n    And they are rich, and ransom all ill deeds.\\n\\n\\n                     35\\n  No more be grieved at that which thou hast done,\\n  Roses have thorns, and silver fountains mud,\\n  Clouds and eclipses stain both moon and sun,\\n  And loathsome canker lives in sweetest bud.\\n  All men make faults, and even I in this,\\n  Authorizing thy trespass with compare,\\n  My self corrupting salving thy amiss,\\n  Excusing thy sins more than thy sins are:\\n  For to thy sensual fault I bring in sense,\\n  Thy adverse party is thy advocate,\\n  And 'gainst my self a lawful plea commence:\\n  Such civil war is in my love and hate,\\n    That I an accessary needs must be,\\n    To that sweet thief which sourly robs from me.\\n\\n\\n                     36  \\n  Let me confess that we two must be twain,\\n  Although our undivided loves are one:\\n  So shall those blots that do with me remain,\\n  Without thy help, by me be borne alone.\\n  In our two loves there is but one respect,\\n  Though in our lives a separable spite,\\n  Which though it alter not love's sole effect,\\n  Yet doth it steal sweet hours from love's delight.\\n  I may not evermore acknowledge thee,\\n  Lest my bewailed guilt should do thee shame,\\n  Nor thou with public kindness honour me,\\n  Unless thou take that honour from thy name:\\n    But do not so, I love thee in such sort,\\n    As thou being mine, mine is thy good report.\\n\\n\\n                     37\\n  As a decrepit father takes delight,\\n  To see his active child do deeds of youth,\\n  So I, made lame by Fortune's dearest spite\\n  Take all my comfort of thy worth and truth.  \\n  For whether beauty, birth, or wealth, or wit,\\n  Or any of these all, or all, or more\\n  Entitled in thy parts, do crowned sit,\\n  I make my love engrafted to this store:\\n  So then I am not lame, poor, nor despised,\\n  Whilst that this shadow doth such substance give,\\n  That I in thy abundance am sufficed,\\n  And by a part of all thy glory live:\\n    Look what is best, that best I wish in thee,\\n    This wish I have, then ten times happy me.\\n\\n\\n                     38\\n  How can my muse want subject to invent\\n  While thou dost breathe that pour'st into my verse,\\n  Thine own sweet argument, too excellent,\\n  For every vulgar paper to rehearse?\\n  O give thy self the thanks if aught in me,\\n  Worthy perusal stand against thy sight,\\n  For who's so dumb that cannot write to thee,\\n  When thou thy self dost give invention light?  \\n  Be thou the tenth Muse, ten times more in worth\\n  Than those old nine which rhymers invocate,\\n  And he that calls on thee, let him bring forth\\n  Eternal numbers to outlive long date.\\n    If my slight muse do please these curious days,\\n    The pain be mine, but thine shall be the praise.\\n\\n\\n                     39\\n  O how thy worth with manners may I sing,\\n  When thou art all the better part of me?\\n  What can mine own praise to mine own self bring:\\n  And what is't but mine own when I praise thee?\\n  Even for this, let us divided live,\\n  And our dear love lose name of single one,\\n  That by this separation I may give:\\n  That due to thee which thou deserv'st alone:\\n  O absence what a torment wouldst thou prove,\\n  Were it not thy sour leisure gave sweet leave,\\n  To entertain the time with thoughts of love,\\n  Which time and thoughts so sweetly doth deceive.  \\n    And that thou teachest how to make one twain,\\n    By praising him here who doth hence remain.\\n\\n\\n                     40\\n  Take all my loves, my love, yea take them all,\\n  What hast thou then more than thou hadst before?\\n  No love, my love, that thou mayst true love call,\\n  All mine was thine, before thou hadst this more:\\n  Then if for my love, thou my love receivest,\\n  I cannot blame thee, for my love thou usest,\\n  But yet be blamed, if thou thy self deceivest\\n  By wilful taste of what thy self refusest.\\n  I do forgive thy robbery gentle thief\\n  Although thou steal thee all my poverty:\\n  And yet love knows it is a greater grief\\n  To bear greater wrong, than hate's known injury.\\n    Lascivious grace, in whom all ill well shows,\\n    Kill me with spites yet we must not be foes.\\n\\n\\n                     41  \\n  Those pretty wrongs that liberty commits,\\n  When I am sometime absent from thy heart,\\n  Thy beauty, and thy years full well befits,\\n  For still temptation follows where thou art.\\n  Gentle thou art, and therefore to be won,\\n  Beauteous thou art, therefore to be assailed.\\n  And when a woman woos, what woman's son,\\n  Will sourly leave her till he have prevailed?\\n  Ay me, but yet thou mightst my seat forbear,\\n  And chide thy beauty, and thy straying youth,\\n  Who lead thee in their riot even there\\n  Where thou art forced to break a twofold truth:\\n    Hers by thy beauty tempting her to thee,\\n    Thine by thy beauty being false to me.\\n\\n\\n                     42\\n  That thou hast her it is not all my grief,\\n  And yet it may be said I loved her dearly,\\n  That she hath thee is of my wailing chief,\\n  A loss in love that touches me more nearly.  \\n  Loving offenders thus I will excuse ye,\\n  Thou dost love her, because thou know'st I love her,\\n  And for my sake even so doth she abuse me,\\n  Suff'ring my friend for my sake to approve her.\\n  If I lose thee, my loss is my love's gain,\\n  And losing her, my friend hath found that loss,\\n  Both find each other, and I lose both twain,\\n  And both for my sake lay on me this cross,\\n    But here's the joy, my friend and I are one,\\n    Sweet flattery, then she loves but me alone.\\n\\n\\n                     43\\n  When most I wink then do mine eyes best see,\\n  For all the day they view things unrespected,\\n  But when I sleep, in dreams they look on thee,\\n  And darkly bright, are bright in dark directed.\\n  Then thou whose shadow shadows doth make bright\\n  How would thy shadow's form, form happy show,\\n  To the clear day with thy much clearer light,\\n  When to unseeing eyes thy shade shines so!  \\n  How would (I say) mine eyes be blessed made,\\n  By looking on thee in the living day,\\n  When in dead night thy fair imperfect shade,\\n  Through heavy sleep on sightless eyes doth stay!\\n    All days are nights to see till I see thee,\\n    And nights bright days when dreams do show thee me.\\n\\n\\n                     44\\n  If the dull substance of my flesh were thought,\\n  Injurious distance should not stop my way,\\n  For then despite of space I would be brought,\\n  From limits far remote, where thou dost stay,\\n  No matter then although my foot did stand\\n  Upon the farthest earth removed from thee,\\n  For nimble thought can jump both sea and land,\\n  As soon as think the place where he would be.\\n  But ah, thought kills me that I am not thought\\n  To leap large lengths of miles when thou art gone,\\n  But that so much of earth and water wrought,\\n  I must attend, time's leisure with my moan.  \\n    Receiving nought by elements so slow,\\n    But heavy tears, badges of either's woe.\\n\\n\\n                     45\\n  The other two, slight air, and purging fire,\\n  Are both with thee, wherever I abide,\\n  The first my thought, the other my desire,\\n  These present-absent with swift motion slide.\\n  For when these quicker elements are gone\\n  In tender embassy of love to thee,\\n  My life being made of four, with two alone,\\n  Sinks down to death, oppressed with melancholy.\\n  Until life's compositio\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itl4Lq-jLuv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e95378e-1345-486d-ab46-25639470b84c"
      },
      "source": [
        "sorted(set(text)) # Return all the unique characters in the text file in a sorted manner. "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '>',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '}']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1isAwH9EN6i7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text)) #contains all the characters in the text  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1RLnjd9Oznc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d70d1632-9188-45ba-93f0-4071ec003912"
      },
      "source": [
        "len(vocab) # count the number of characters in the text file. This will be used at the output layer of the RNN"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Hj_KmFO1fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "403f50d2-b8fd-45aa-991e-33527aee1b79"
      },
      "source": [
        "for pair in enumerate(vocab):\n",
        "  print (pair)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '\\n')\n",
            "(1, ' ')\n",
            "(2, '!')\n",
            "(3, '\"')\n",
            "(4, '&')\n",
            "(5, \"'\")\n",
            "(6, '(')\n",
            "(7, ')')\n",
            "(8, ',')\n",
            "(9, '-')\n",
            "(10, '.')\n",
            "(11, '0')\n",
            "(12, '1')\n",
            "(13, '2')\n",
            "(14, '3')\n",
            "(15, '4')\n",
            "(16, '5')\n",
            "(17, '6')\n",
            "(18, '7')\n",
            "(19, '8')\n",
            "(20, '9')\n",
            "(21, ':')\n",
            "(22, ';')\n",
            "(23, '<')\n",
            "(24, '>')\n",
            "(25, '?')\n",
            "(26, 'A')\n",
            "(27, 'B')\n",
            "(28, 'C')\n",
            "(29, 'D')\n",
            "(30, 'E')\n",
            "(31, 'F')\n",
            "(32, 'G')\n",
            "(33, 'H')\n",
            "(34, 'I')\n",
            "(35, 'J')\n",
            "(36, 'K')\n",
            "(37, 'L')\n",
            "(38, 'M')\n",
            "(39, 'N')\n",
            "(40, 'O')\n",
            "(41, 'P')\n",
            "(42, 'Q')\n",
            "(43, 'R')\n",
            "(44, 'S')\n",
            "(45, 'T')\n",
            "(46, 'U')\n",
            "(47, 'V')\n",
            "(48, 'W')\n",
            "(49, 'X')\n",
            "(50, 'Y')\n",
            "(51, 'Z')\n",
            "(52, '[')\n",
            "(53, ']')\n",
            "(54, '_')\n",
            "(55, '`')\n",
            "(56, 'a')\n",
            "(57, 'b')\n",
            "(58, 'c')\n",
            "(59, 'd')\n",
            "(60, 'e')\n",
            "(61, 'f')\n",
            "(62, 'g')\n",
            "(63, 'h')\n",
            "(64, 'i')\n",
            "(65, 'j')\n",
            "(66, 'k')\n",
            "(67, 'l')\n",
            "(68, 'm')\n",
            "(69, 'n')\n",
            "(70, 'o')\n",
            "(71, 'p')\n",
            "(72, 'q')\n",
            "(73, 'r')\n",
            "(74, 's')\n",
            "(75, 't')\n",
            "(76, 'u')\n",
            "(77, 'v')\n",
            "(78, 'w')\n",
            "(79, 'x')\n",
            "(80, 'y')\n",
            "(81, 'z')\n",
            "(82, '|')\n",
            "(83, '}')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ki-LwF5P4-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOHCmITxQpGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind_to_char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX8T7drqRkOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dab4109a-61a7-4950-aa1b-3693c7f0f774"
      },
      "source": [
        "ind_to_char[80]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'y'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjCcYQOSK4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU43ITALSqbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61e3e42f-9794-424c-c928-98c891d45c10"
      },
      "source": [
        "encoded_text[:3]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIc6dwK0TCxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c4b73749-4fb5-4634-b41c-9424dd07d1a8"
      },
      "source": [
        "text[2:200]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"                    1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Bokmm3URZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "56c655b8-2ff3-4092-d60a-7963f792f078"
      },
      "source": [
        "encoded_text[2:200]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64, 73, 60,\n",
              "       74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,  1, 59,\n",
              "       60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,  0,  1,\n",
              "        1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57, 60, 56,\n",
              "       76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64, 62, 63, 75,  1,\n",
              "       69, 60, 77, 60, 73,  1, 59, 64, 60,  8,  0,  1,  1, 27, 76, 75,  1,\n",
              "       56, 74,  1, 75, 63, 60,  1, 73, 64, 71, 60, 73,  1, 74, 63, 70, 76,\n",
              "       67, 59,  1, 57, 80,  1, 75, 64, 68, 60,  1, 59, 60, 58, 60, 56, 74,\n",
              "       60,  8,  0,  1,  1, 33, 64, 74,  1, 75, 60, 69, 59, 60, 73,  1, 63,\n",
              "       60, 64, 73,  1, 68, 64, 62, 63, 75,  1, 57, 60, 56, 73,  1, 63, 64,\n",
              "       74,  1, 68, 60, 68, 70, 73, 80, 21,  0,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSPnKqR9dBuT",
        "colab_type": "text"
      },
      "source": [
        "### Defining the sequence length from the lines. To find the structure of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmgK0sNUW8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "56f2e65b-b430-4b34-8650-39de97370550"
      },
      "source": [
        "print (text[:300])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzcKdbEDctea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line = \"From fairest creatures we desire increase\" #Taking a single line \n",
        "lines = '''\n",
        "From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ivyBjIdQ7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eb0aa286-66a0-4824-a93c-565847004725"
      },
      "source": [
        "print (len(line))\n",
        "print (len (lines))\n",
        "# Here a structured line from the poem or suanet here is of 133 line hence taking it to be of 120 line. \n",
        "#This can change for different type of texts. A story or an email can have different sequence legnth"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41\n",
            "132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KP-HFC7dRrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 120"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3kPE3XeCjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c813cf7-e4c1-40e3-e156-67fc8c77aaf1"
      },
      "source": [
        "total_num_sep = print (len(text)// seq_len + 1)  # Finding the total sequences in the text. Also using + 1 as 0 is the indexing. "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH-0X0-xeO1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_dateset = tf.data.Dataset.from_tensor_slices(encoded_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MAFJMc5fWdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a7ed5e-0a8c-4f5f-d167-fdc378604c5e"
      },
      "source": [
        "type(char_dateset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmJPZk8PfZI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aac221f6-6cfa-43e8-e5c9-84477c472963"
      },
      "source": [
        "print (char_dateset)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDqmKQRcfcDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af8a3e4a-dbc5-4f92-ec75-dbd2c7495e8d"
      },
      "source": [
        "for item in char_dateset.take(500):\n",
        "  print(ind_to_char[item.numpy()])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "1\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "F\n",
            "r\n",
            "o\n",
            "m\n",
            " \n",
            "f\n",
            "a\n",
            "i\n",
            "r\n",
            "e\n",
            "s\n",
            "t\n",
            " \n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "t\n",
            "u\n",
            "r\n",
            "e\n",
            "s\n",
            " \n",
            "w\n",
            "e\n",
            " \n",
            "d\n",
            "e\n",
            "s\n",
            "i\n",
            "r\n",
            "e\n",
            " \n",
            "i\n",
            "n\n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "s\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            "b\n",
            "y\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "u\n",
            "t\n",
            "y\n",
            "'\n",
            "s\n",
            " \n",
            "r\n",
            "o\n",
            "s\n",
            "e\n",
            " \n",
            "m\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "n\n",
            "e\n",
            "v\n",
            "e\n",
            "r\n",
            " \n",
            "d\n",
            "i\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "B\n",
            "u\n",
            "t\n",
            " \n",
            "a\n",
            "s\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "r\n",
            "i\n",
            "p\n",
            "e\n",
            "r\n",
            " \n",
            "s\n",
            "h\n",
            "o\n",
            "u\n",
            "l\n",
            "d\n",
            " \n",
            "b\n",
            "y\n",
            " \n",
            "t\n",
            "i\n",
            "m\n",
            "e\n",
            " \n",
            "d\n",
            "e\n",
            "c\n",
            "e\n",
            "a\n",
            "s\n",
            "e\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "H\n",
            "i\n",
            "s\n",
            " \n",
            "t\n",
            "e\n",
            "n\n",
            "d\n",
            "e\n",
            "r\n",
            " \n",
            "h\n",
            "e\n",
            "i\n",
            "r\n",
            " \n",
            "m\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "b\n",
            "e\n",
            "a\n",
            "r\n",
            " \n",
            "h\n",
            "i\n",
            "s\n",
            " \n",
            "m\n",
            "e\n",
            "m\n",
            "o\n",
            "r\n",
            "y\n",
            ":\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "B\n",
            "u\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "o\n",
            "u\n",
            " \n",
            "c\n",
            "o\n",
            "n\n",
            "t\n",
            "r\n",
            "a\n",
            "c\n",
            "t\n",
            "e\n",
            "d\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "b\n",
            "r\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "e\n",
            "y\n",
            "e\n",
            "s\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "F\n",
            "e\n",
            "e\n",
            "d\n",
            "'\n",
            "s\n",
            "t\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "l\n",
            "i\n",
            "g\n",
            "h\n",
            "t\n",
            "'\n",
            "s\n",
            " \n",
            "f\n",
            "l\n",
            "a\n",
            "m\n",
            "e\n",
            " \n",
            "w\n",
            "i\n",
            "t\n",
            "h\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            "-\n",
            "s\n",
            "u\n",
            "b\n",
            "s\n",
            "t\n",
            "a\n",
            "n\n",
            "t\n",
            "i\n",
            "a\n",
            "l\n",
            " \n",
            "f\n",
            "u\n",
            "e\n",
            "l\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "M\n",
            "a\n",
            "k\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "a\n",
            " \n",
            "f\n",
            "a\n",
            "m\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "w\n",
            "h\n",
            "e\n",
            "r\n",
            "e\n",
            " \n",
            "a\n",
            "b\n",
            "u\n",
            "n\n",
            "d\n",
            "a\n",
            "n\n",
            "c\n",
            "e\n",
            " \n",
            "l\n",
            "i\n",
            "e\n",
            "s\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "y\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "f\n",
            "o\n",
            "e\n",
            ",\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "y\n",
            " \n",
            "s\n",
            "w\n",
            "e\n",
            "e\n",
            "t\n",
            " \n",
            "s\n",
            "e\n",
            "l\n",
            "f\n",
            " \n",
            "t\n",
            "o\n",
            "o\n",
            " \n",
            "c\n",
            "r\n",
            "u\n",
            "e\n",
            "l\n",
            ":\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "T\n",
            "h\n",
            "o\n",
            "u\n",
            " \n",
            "t\n",
            "h\n",
            "a\n",
            "t\n",
            " \n",
            "a\n",
            "r\n",
            "t\n",
            " \n",
            "n\n",
            "o\n",
            "w\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "w\n",
            "o\n",
            "r\n",
            "l\n",
            "d\n",
            "'\n",
            "s\n",
            " \n",
            "f\n",
            "r\n",
            "e\n",
            "s\n",
            "h\n",
            " \n",
            "o\n",
            "r\n",
            "n\n",
            "a\n",
            "m\n",
            "e\n",
            "n\n",
            "t\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "A\n",
            "n\n",
            "d\n",
            " \n",
            "o\n",
            "n\n",
            "l\n",
            "y\n",
            " \n",
            "h\n",
            "e\n",
            "r\n",
            "a\n",
            "l\n",
            "d\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "t\n",
            "h\n",
            "e\n",
            " \n",
            "g\n",
            "a\n",
            "u\n",
            "d\n",
            "y\n",
            " \n",
            "s\n",
            "p\n",
            "r\n",
            "i\n",
            "n\n",
            "g\n",
            ",\n",
            "\n",
            "\n",
            " \n",
            " \n",
            "W\n",
            "i\n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            " \n",
            "t\n",
            "h\n",
            "i\n",
            "n\n",
            "e\n",
            " \n",
            "o\n",
            "w\n",
            "n\n",
            " \n",
            "b\n",
            "u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHvloL1dgQuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dateset.batch(batch_size=seq_len+1,drop_remainder=True)\n",
        "#creating a sequence batches of length 121 from the dataset. Dropping the remaining characters from the text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYLpitm6I9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, creating a target sequences from sequence above by shifting each batch by +1 and storing in tuple object."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxLGPitr6KDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "  input_txt = seq[:-1] # \"Hello my nam...\"\n",
        "  output_txt = seq[1:] # \"ello my name....\"\n",
        "  return input_txt,output_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNMfhMi8928v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5chKYg0898Ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "a125c1b8-7b41-4d50-e841-ddafc84ca378"
      },
      "source": [
        "for input_txt, target_txt in dataset.take(1):\n",
        "  print(input_txt.numpy())\n",
        "  print (\"\".join(ind_to_char[input_txt.numpy()])) ### This is the input sequence \n",
        "  print ('\\n')\n",
        "  print (target_txt.numpy())\n",
        "  print (\"\".join(ind_to_char[target_txt.numpy()])) # This is the target sequence \n",
        "  print ('\\n')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "\n",
            "\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4V_fkpYhf1a",
        "colab_type": "text"
      },
      "source": [
        "### Next target is to create or generate the Training sequence. \n",
        "Batches and shuffle. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WakxkRTs-zRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "batch_size = 128\n",
        "\n",
        "# Need to define the buffer size for also for randomamising. Else all the text in the memory can be randsomised.\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6xLcW7VkerQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42f0a245-9e6a-47ee-9885-a8d126edf52a"
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.BatchDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je0KN3AClDCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "rnn_neurons=1026\n",
        "embeded_dim=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhgkLcIjmVYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CXifrLimNE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpZLL0xJm6lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y-CJZgcmrcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocab_size,embeded_dim,rnn_neurons,batch_size):\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size,embeded_dim,batch_input_shape=[batch_size,None]))\n",
        "  model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,\n",
        "              recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  model.add(Dense(vocab_size))\n",
        "\n",
        "  model.compile('adam',loss=sparse_cat_loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mAyaIzBpes_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(vocab_size=vocab_size,embeded_dim=embeded_dim,rnn_neurons=rnn_neurons,\n",
        "                     batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaP3OJwrpolS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "064a73d4-ce71-466b-ba7c-fb119abcd1d4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 64)           5376      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 84)           86268     \n",
            "=================================================================\n",
            "Total params: 3,452,820\n",
            "Trainable params: 3,452,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-cfK2doefQl",
        "colab_type": "text"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-ONd-OMpwoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  #input_example_batch is the first sequence of letters\n",
        "  #target_example_batch is ths initial sequence shifted just one  \n",
        "  example_batch_predections = model(input_example_batch)\n",
        "  #print (tf.shape(input_example_batch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUSs9QCNfVFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "725ecf1a-71e6-4017-c159-76882a6949e9"
      },
      "source": [
        "example_batch_predections.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 120, 84])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2-6J_wm_hH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "4230751d-aa20-4da0-e052-551ba058e2b1"
      },
      "source": [
        "example_batch_predections[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(120, 84), dtype=float32, numpy=\n",
              "array([[ 2.0521251e-03, -1.7755585e-03,  2.6054389e-03, ...,\n",
              "        -2.5667967e-03,  1.2200433e-03,  2.8477554e-04],\n",
              "       [-3.6186865e-03, -3.5389732e-03,  3.9515942e-03, ...,\n",
              "        -2.2552307e-03,  2.6777254e-03, -4.6404093e-03],\n",
              "       [ 1.1694422e-03,  1.6424228e-03,  1.4082689e-03, ...,\n",
              "         6.2749642e-03,  3.1601158e-03,  5.4023024e-03],\n",
              "       ...,\n",
              "       [-5.6377337e-03, -2.0983333e-03,  4.1400217e-03, ...,\n",
              "         2.9928416e-03,  1.5711214e-03, -6.0065882e-04],\n",
              "       [-9.9164899e-04, -9.2885401e-03,  4.6205269e-03, ...,\n",
              "        -4.6009896e-05, -1.8227848e-03,  2.6598664e-03],\n",
              "       [-2.4708356e-03, -4.1519394e-03,  7.8212107e-03, ...,\n",
              "         7.4694334e-03, -2.3135873e-03,  3.3908251e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R24boWPUn0sS",
        "colab_type": "text"
      },
      "source": [
        "## Grabbing the indices from the tensor and finding the sequence of number generated by the predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V58TXMUpnCgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sample_indices = tf.random.categorical(example_batch_predections[0],num_samples=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy5ClFZUoC2m",
        "colab_type": "text"
      },
      "source": [
        "## Next with the shape of this tensor, we need to convert it to an array to check out the char by passing in ind_to_char array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DlF93fDnPQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_indices = tf.squeeze(sample_indices,axis =-1).numpy()\n",
        "## reshaping into a numpy array by using squeeze method"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv3Y9ON9nViw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "33a6ca71-ec37-44e2-f8c1-c01ae139a16c"
      },
      "source": [
        "sample_indices"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37, 20, 32, 24, 57, 55,  7, 24, 80, 72, 15, 81, 25, 23, 30, 45, 53,\n",
              "        9, 65, 55, 19, 82, 42, 34, 83, 35, 11, 41, 78, 34, 60, 49, 14,  0,\n",
              "        0, 50, 19, 13, 80, 31, 67, 83, 51, 12,  0,  7, 78, 45, 25,  4, 74,\n",
              "       29, 41, 42, 52, 58, 62, 19, 79,  3, 70, 58, 44, 77,  8, 74, 34, 19,\n",
              "       73, 49, 40, 79,  2, 70, 70, 48, 54, 74, 26, 13, 45,  0, 22, 50, 80,\n",
              "       81, 28, 48, 26, 64, 22, 34, 27, 24, 29, 33, 25, 76, 63, 15, 44, 68,\n",
              "       14, 58, 63, 66, 57, 77, 11, 24, 79,  7, 74, 43,  8, 48, 65, 51, 80,\n",
              "       18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XZo5R2lolK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "78c4ddb4-9d79-4b15-f0b4-133f2a39110f"
      },
      "source": [
        "ind_to_char[sample_indices]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['L', '9', 'G', '>', 'b', '`', ')', '>', 'y', 'q', '4', 'z', '?',\n",
              "       '<', 'E', 'T', ']', '-', 'j', '`', '8', '|', 'Q', 'I', '}', 'J',\n",
              "       '0', 'P', 'w', 'I', 'e', 'X', '3', '\\n', '\\n', 'Y', '8', '2', 'y',\n",
              "       'F', 'l', '}', 'Z', '1', '\\n', ')', 'w', 'T', '?', '&', 's', 'D',\n",
              "       'P', 'Q', '[', 'c', 'g', '8', 'x', '\"', 'o', 'c', 'S', 'v', ',',\n",
              "       's', 'I', '8', 'r', 'X', 'O', 'x', '!', 'o', 'o', 'W', '_', 's',\n",
              "       'A', '2', 'T', '\\n', ';', 'Y', 'y', 'z', 'C', 'W', 'A', 'i', ';',\n",
              "       'I', 'B', '>', 'D', 'H', '?', 'u', 'h', '4', 'S', 'm', '3', 'c',\n",
              "       'h', 'k', 'b', 'v', '0', '>', 'x', ')', 's', 'R', ',', 'W', 'j',\n",
              "       'Z', 'y', '7'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRRMC3ANhH0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qf2iglTSMdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## NEw code\n",
        "\n",
        "import os \n",
        "checkpoint_path = \"training_1\\cp.ckpt\" \n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        " # Create checkpoint  callback \n",
        "checkpoint_callback =tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_weights_only=True,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpgvBFPopWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4F3bClpH-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ece50737-f3b5-4f35-d363-a65d92b02f7a"
      },
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 351 steps\n",
            "351/351 [==============================] - 44s 125ms/step - loss: 1.7372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f83500f39e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUOIq6b1hz9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GyOXSvjf8-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "model = create_model(vocab_size=vocab_size,embeded_dim=embeded_dim,rnn_neurons=rnn_neurons,\n",
        "                     batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v1wMkfaWZ6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "93288f66-d9c6-4c41-d958-e99cb0331ecf"
      },
      "source": [
        "model.load_weights(model)\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-1413f48d6a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1181\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   1501\u001b[0m           filepath.endswith('.hdf5'))\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1pd8GPnf87u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "e07b308f-a2fc-4d00-e5f0-cd66c4afb5ae"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (128, None, 64)           5376      \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (128, None, 84)           86268     \n",
            "=================================================================\n",
            "Total params: 3,452,820\n",
            "Trainable params: 3,452,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQUwwFSpMhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters.\n",
        "  '''\n",
        "\n",
        "  # defining the number of elements we want to predict\n",
        "  num_generate = gen_size\n",
        "  # converting the charater in starting seed string into characters.\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "  \n",
        "  #changing the dimentsion so that its in right shape for our model.'\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "  #empty list to append the text generated by the model\n",
        "  text_generated = []\n",
        "\n",
        "  #Need to set the temperature for the randomness in our model.\n",
        "  #The temeprerature divdes the logits which will be used for the softmax function before passing onto the next\n",
        "  temperature = temp\n",
        "\n",
        "  #Resetting the state of model before calling calling the predection clears the hidden layer states.\n",
        "  model.reset_states()\n",
        "\n",
        "  #Looping in through each of the characters\n",
        "  for i in range(num_generate): \n",
        "\n",
        "    prediction = model(input_eval)\n",
        "\n",
        "    #squezzing to undo the expanding dimensions domne above.\n",
        "    prediction = tf.squeeze(prediction,0)\n",
        "\n",
        "    #dividing the logits\n",
        "    prediction = prediction/temperature\n",
        "\n",
        "    #finding the single digit from the tensor to find the index and pass ot character array to find the real letters.\n",
        "    #use function random.categorical\n",
        "\n",
        "    predicted_id = tf.random.categorical(prediction,num_samples=1)[-1:0].numpy()\n",
        "\n",
        "    #Expand the predected text \n",
        "    input_eval = tf.expand_dims(predicted_id,0)\n",
        "\n",
        "    #append into the list\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + \"\".join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB4JIQXJP4f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model,\"BRUTU\",gen_size=500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9AeM30YQHHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
        "  time series problems.\n",
        "  '''\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  # Temperature effects randomness in our resulting text\n",
        "  # The term is derived from entropy/thermodynamics.\n",
        "  # The temperature is used to effect probability of next characters.\n",
        "  # Higher probability == lesss surprising/ more expected\n",
        "  # Lower temperature == more surprising / less expected\n",
        " \n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      print (tf.shape(input_eval))\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHeYi6tZQ5Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model,\"BRUTU\",gen_size=500))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pz3EUO_Q9qb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}